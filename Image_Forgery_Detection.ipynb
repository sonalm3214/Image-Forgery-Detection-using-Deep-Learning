{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Set seeds for reproducibility\n",
    "import random\n",
    "random.seed(69)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(69)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(69)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Allow memory growth on the GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Check if GPU is available\n",
    "gpu_available = tf.config.list_physical_devices('GPU')\n",
    "print(\"Available GPUs:\", gpu_available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "import re  # Import the regular expression module\n",
    "\n",
    "# Clear session and garbage collection\n",
    "K.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:  # Check if any GPUs are found\n",
    "        gpu_device_name = gpus[0].name  # Get the full device name\n",
    "        match = re.search(r'GPU:(\\d+)', gpu_device_name)  # Extract \"GPU:X\"\n",
    "        if match:\n",
    "            short_gpu_name = match.group(0)  # This will be \"GPU:0\", \"GPU:1\", etc.\n",
    "            tf.config.experimental.reset_memory_stats(short_gpu_name)\n",
    "            print(f\"Memory stats reset for device: {short_gpu_name}\")\n",
    "        else:\n",
    "            print(\"Could not parse GPU device name.\")\n",
    "    else:\n",
    "        print(\"No GPUs found. Skipping memory reset.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during memory reset: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, regularizers, backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard, CSVLogger, TerminateOnNaN\n",
    ")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, precision_recall_curve\n",
    "from sklearn.utils import class_weight, compute_class_weight, shuffle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WWWZX\\AI_Class_Work\\cnn_split\\casia2data\n",
      "File exists!\n",
      "File exists!\n",
      "File exists!\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "# Data Directories (REPLACE THESE WITH YOUR ACTUAL PATHS)\n",
    "base_dir = os.getcwd()  # Contains subfolders for each class\n",
    "train_dir = base_dir + \"/train_val_test_split_384x384/train\"\n",
    "val_dir = base_dir + \"/train_val_test_split_384x384/val\"\n",
    "test_dir = base_dir + \"/train_val_test_split_384x384/test\"\n",
    "if os.path.exists(train_dir):\n",
    "    print(\"File exists!\")\n",
    "else:\n",
    "    print(\"File NOT found!\")\n",
    "if os.path.exists(val_dir):\n",
    "    print(\"File exists!\")\n",
    "else:\n",
    "    print(\"File NOT found!\")\n",
    "if os.path.exists(test_dir):\n",
    "    print(\"File exists!\")\n",
    "else:\n",
    "    print(\"File NOT found!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure your original convert_to_ela_image function is defined somewhere:\n",
    "def convert_to_ela_image(image_path, quality=90, img_size=256):\n",
    "    \"\"\"Converts an image to its ELA representation using file-based processing.\"\"\"\n",
    "    temp_filename = \"temp_ela.jpg\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image.save(temp_filename, \"JPEG\", quality=quality)\n",
    "    temp_image = Image.open(temp_filename)\n",
    "    \n",
    "    ela_image = ImageChops.difference(image, temp_image)\n",
    "    extrema = ela_image.getextrema()\n",
    "    max_diff = max([ex[1] for ex in extrema])\n",
    "    scale = 255.0 / max_diff if max_diff != 0 else 1\n",
    "    ela_image = ImageEnhance.Brightness(ela_image).enhance(scale)\n",
    "    \n",
    "    ela_image = ela_image.resize((256, 256))\n",
    "    return np.array(ela_image) / 255.0  # Normalize to [0,1]\n",
    "\n",
    "class ELAImageDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, directory, batch_size, img_size, class_mode=\"binary\", shuffle=True):\n",
    "        self.directory = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.class_mode = class_mode\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        self.classes = {\"au\": 0, \"tp\": 1}  # Assign class labels\n",
    "        self.image_paths, self.labels = self._load_image_paths_and_labels()\n",
    "        self.labels = list(self.labels)  # Ensure labels are a list\n",
    "        self.on_epoch_end()  # Shuffle initially if needed\n",
    "    \n",
    "    def _load_image_paths_and_labels(self):\n",
    "        \"\"\"Loads all image paths and labels from the directory.\"\"\"\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        for class_name, label in self.classes.items():\n",
    "            class_path = os.path.join(self.directory, class_name)\n",
    "            if os.path.exists(class_path):\n",
    "                for filename in os.listdir(class_path):\n",
    "                    if filename.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "                        image_paths.append(os.path.join(class_path, filename))\n",
    "                        labels.append(label)\n",
    "        return image_paths, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of batches (including any partial batch).\"\"\"\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates one batch of data.\"\"\"\n",
    "        start_idx = index * self.batch_size\n",
    "        end_idx = min((index + 1) * self.batch_size, len(self.image_paths))\n",
    "        \n",
    "        batch_image_paths = self.image_paths[start_idx:end_idx]\n",
    "        batch_labels = self.labels[start_idx:end_idx]\n",
    "        \n",
    "        X, Y = self._generate_batch(batch_image_paths, batch_labels)\n",
    "        return X, Y\n",
    "\n",
    "    def _generate_batch(self, batch_image_paths, batch_labels):\n",
    "        \"\"\"Generates a batch of images (converted to ELA) and corresponding labels.\"\"\"\n",
    "        # Apply ELA conversion to each image in the batch.\n",
    "        batch_images = [convert_to_ela_image(img_path, quality=90, img_size=self.img_size) \n",
    "                        for img_path in batch_image_paths]\n",
    "        return np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Shuffles the dataset after each epoch.\"\"\"\n",
    "        if self.shuffle:\n",
    "            combined = list(zip(self.image_paths, self.labels))\n",
    "            np.random.shuffle(combined)\n",
    "            self.image_paths, self.labels = zip(*combined)\n",
    "            # Convert back to list and array respectively:\n",
    "            self.image_paths = list(self.image_paths)\n",
    "            self.labels = list(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Image Parameters\n",
    "img_size = 384  # Or your desired size\n",
    "batch_size = 32 # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELA-based data generators are ready!\n"
     ]
    }
   ],
   "source": [
    "# Use Custom ELA Data Generators\n",
    "train_generator = ELAImageDataGenerator(directory=train_dir, batch_size=batch_size, img_size=img_size)\n",
    "val_generator = ELAImageDataGenerator(directory=val_dir, batch_size=batch_size, img_size=img_size)\n",
    "test_generator = ELAImageDataGenerator(directory=test_dir, batch_size=batch_size, img_size=img_size, shuffle=False)\n",
    "print(\"ELA-based data generators are ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#for debugging purpose only\n",
    "# batch_images, batch_labels = train_generator[0]  # Assuming train_generator is now ELA-based\n",
    "# print(\"Min pixel value:\", np.min(batch_images))\n",
    "# print(\"Max pixel value:\", np.max(batch_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Class Distribution: {'au': 4498, 'tp': 965}\n",
      "Validation Class Distribution: {'au': 964, 'tp': 206}\n",
      "Test Class Distribution: {'au': 976, 'tp': 208}\n"
     ]
    }
   ],
   "source": [
    "#Define your class mapping manually\n",
    "inverse_class_map = {0: \"au\", 1: \"tp\"}  # Adjust this according to your dataset\n",
    "\n",
    "# Get class distributions for training set\n",
    "unique_classes, class_counts = np.unique(train_generator.labels, return_counts=True)\n",
    "class_labels = [inverse_class_map[i] for i in unique_classes]\n",
    "print(\"Train Class Distribution:\", dict(zip(class_labels, class_counts)))\n",
    "\n",
    "# Get class distributions for validation set\n",
    "unique_classes, class_counts = np.unique(val_generator.labels, return_counts=True)\n",
    "class_labels = [inverse_class_map[i] for i in unique_classes]\n",
    "print(\"Validation Class Distribution:\", dict(zip(class_labels, class_counts)))\n",
    "\n",
    "# Get class distributions for test set\n",
    "unique_classes, class_counts = np.unique(test_generator.labels, return_counts=True)\n",
    "class_labels = [inverse_class_map[i] for i in unique_classes]\n",
    "print(\"Test Class Distribution:\", dict(zip(class_labels, class_counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#for debugging purposes\n",
    "# print(dir(train_generator))  # See available attributes\n",
    "# print(type(train_generator.classes))  # Should be list or np.ndarray\n",
    "# print(train_generator.classes)  # See what it contains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#for debugging purpose only\n",
    "# # Check if 'labels' is accessible and not empty\n",
    "# if hasattr(train_generator, \"labels\") and train_generator.labels:\n",
    "#     train_classes = np.array(train_generator.labels)  # Extract labels\n",
    "#     print(\"Extracted Labels:\", train_classes[:10])  # Print first 10 labels\n",
    "# else:\n",
    "#     raise AttributeError(\"train_generator does not have a valid 'labels' attribute.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Class Weights: {0: 0.6072698977323254, 1: 2.8305699481865285}\n"
     ]
    }
   ],
   "source": [
    "# Ensure train_classes is correctly extracted\n",
    "train_classes = np.array(train_generator.labels)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight='balanced', \n",
    "    classes=np.unique(train_classes), \n",
    "    y=train_classes\n",
    ")\n",
    "\n",
    "class_weight_dict = dict(zip(np.unique(train_classes), class_weights))\n",
    "print(\"Computed Class Weights:\", class_weight_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VGG16 with ImageNet weights, without the top classification layer\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 384, 384, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 384, 384, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 384, 384, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 192, 192, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 192, 192, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 192, 192, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 96, 96, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 96, 96, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 96, 96, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 96, 96, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 96, 96, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 48, 48, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 48, 48, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 48, 48, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 48, 48, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 48, 48, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 24, 24, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 24, 24, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 24, 24, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 24, 24, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 24, 24, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 12, 12, 512)       0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 0\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.38),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(alpha=0.5, gamma=3.0):\n",
    "    def loss(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, K.floatx())  # Convert labels to float32\n",
    "        epsilon = K.epsilon()  # Small value to avoid log(0)\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)  # Clip predictions\n",
    "\n",
    "        bce = K.binary_crossentropy(y_true, y_pred)  # Compute binary cross-entropy\n",
    "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)  # Probabilities\n",
    "        focal_factor = alpha * K.pow(1 - p_t, gamma)  # Focal weighting\n",
    "\n",
    "        return K.mean(focal_factor * bce)  # Weighted BCE\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success of this block\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=focal_loss(alpha=0.25, gamma=2.0),  \n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"success of this block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success of this block\n"
     ]
    }
   ],
   "source": [
    "# Callbacks (Early Stopping and ReduceLROnPlateau)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True,verbose=2) #stop training when validation loss is not decreasing\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.35, patience=5, min_lr=1e-8,verbose=2) #reduce learning rate when validation loss is not decreasing\n",
    "checkpoint_callback = ModelCheckpoint(filepath='checkpoints/model_{epoch:02d}.h5', monitor='val_loss', save_best_only=True, save_weights_only=False, verbose=2)\n",
    "tensorboard_callback = TensorBoard(log_dir=\"Tensor_Board_Logs\", histogram_freq=1,)\n",
    "csv_logger = CSVLogger('training_log.csv')\n",
    "nan_terminator = TerminateOnNaN()\n",
    "\n",
    "print(\"success of this block\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.7591 - precision: 0.4125 - recall: 0.8570           \n",
      "Epoch 1: val_loss improved from inf to 0.02699, saving model to checkpoints\\model_01.h5\n",
      "171/171 [==============================] - 74s 365ms/step - loss: 0.0348 - accuracy: 0.7591 - precision: 0.4125 - recall: 0.8570 - val_loss: 0.0270 - val_accuracy: 0.8239 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.8742 - precision: 0.6153 - recall: 0.7689          \n",
      "Epoch 2: val_loss improved from 0.02699 to 0.02410, saving model to checkpoints\\model_02.h5\n",
      "171/171 [==============================] - 59s 342ms/step - loss: 0.0215 - accuracy: 0.8742 - precision: 0.6153 - recall: 0.7689 - val_loss: 0.0241 - val_accuracy: 0.8239 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9041 - precision: 0.7243 - recall: 0.7378  \n",
      "Epoch 3: val_loss improved from 0.02410 to 0.01582, saving model to checkpoints\\model_03.h5\n",
      "171/171 [==============================] - 58s 340ms/step - loss: 0.0169 - accuracy: 0.9041 - precision: 0.7243 - recall: 0.7378 - val_loss: 0.0158 - val_accuracy: 0.9009 - val_precision: 0.9592 - val_recall: 0.4563 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9162 - precision: 0.7723 - recall: 0.7451  \n",
      "Epoch 4: val_loss improved from 0.01582 to 0.01244, saving model to checkpoints\\model_04.h5\n",
      "171/171 [==============================] - 60s 347ms/step - loss: 0.0152 - accuracy: 0.9162 - precision: 0.7723 - recall: 0.7451 - val_loss: 0.0124 - val_accuracy: 0.9308 - val_precision: 0.8788 - val_recall: 0.7039 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9196 - precision: 0.7859 - recall: 0.7492  \n",
      "Epoch 5: val_loss improved from 0.01244 to 0.01178, saving model to checkpoints\\model_05.h5\n",
      "171/171 [==============================] - 65s 379ms/step - loss: 0.0150 - accuracy: 0.9196 - precision: 0.7859 - recall: 0.7492 - val_loss: 0.0118 - val_accuracy: 0.9376 - val_precision: 0.8182 - val_recall: 0.8301 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9228 - precision: 0.7967 - recall: 0.7554  \n",
      "Epoch 6: val_loss did not improve from 0.01178\n",
      "171/171 [==============================] - 59s 342ms/step - loss: 0.0148 - accuracy: 0.9228 - precision: 0.7967 - recall: 0.7554 - val_loss: 0.0281 - val_accuracy: 0.8667 - val_precision: 0.9808 - val_recall: 0.2476 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0138 - accuracy: 0.9259 - precision: 0.8111 - recall: 0.7565  \n",
      "Epoch 7: val_loss did not improve from 0.01178\n",
      "171/171 [==============================] - 64s 375ms/step - loss: 0.0138 - accuracy: 0.9259 - precision: 0.8111 - recall: 0.7565 - val_loss: 0.0160 - val_accuracy: 0.9162 - val_precision: 0.7061 - val_recall: 0.8981 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9295 - precision: 0.8132 - recall: 0.7803  \n",
      "Epoch 8: val_loss did not improve from 0.01178\n",
      "171/171 [==============================] - 64s 375ms/step - loss: 0.0137 - accuracy: 0.9295 - precision: 0.8132 - recall: 0.7803 - val_loss: 0.0277 - val_accuracy: 0.8419 - val_precision: 0.5280 - val_recall: 0.9612 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0135 - accuracy: 0.9293 - precision: 0.8130 - recall: 0.7793  \n",
      "Epoch 9: val_loss did not improve from 0.01178\n",
      "171/171 [==============================] - 61s 356ms/step - loss: 0.0135 - accuracy: 0.9293 - precision: 0.8130 - recall: 0.7793 - val_loss: 0.0415 - val_accuracy: 0.7521 - val_precision: 0.4139 - val_recall: 0.9806 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0136 - accuracy: 0.9277 - precision: 0.8098 - recall: 0.7720  \n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 3.499999911582563e-05.\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.01178\n",
      "171/171 [==============================] - 60s 349ms/step - loss: 0.0136 - accuracy: 0.9277 - precision: 0.8098 - recall: 0.7720 - val_loss: 0.0385 - val_accuracy: 0.7821 - val_precision: 0.4457 - val_recall: 0.9757 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0123 - accuracy: 0.9334 - precision: 0.8200 - recall: 0.7979  \n",
      "Epoch 11: val_loss improved from 0.01178 to 0.01100, saving model to checkpoints\\model_11.h5\n",
      "171/171 [==============================] - 64s 371ms/step - loss: 0.0123 - accuracy: 0.9334 - precision: 0.8200 - recall: 0.7979 - val_loss: 0.0110 - val_accuracy: 0.9427 - val_precision: 0.8677 - val_recall: 0.7961 - lr: 3.5000e-05\n",
      "Epoch 12/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9356 - precision: 0.8357 - recall: 0.7907  \n",
      "Epoch 12: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 63s 368ms/step - loss: 0.0126 - accuracy: 0.9356 - precision: 0.8357 - recall: 0.7907 - val_loss: 0.0113 - val_accuracy: 0.9427 - val_precision: 0.9017 - val_recall: 0.7573 - lr: 3.5000e-05\n",
      "Epoch 13/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9319 - precision: 0.8178 - recall: 0.7907  \n",
      "Epoch 13: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 62s 362ms/step - loss: 0.0126 - accuracy: 0.9319 - precision: 0.8178 - recall: 0.7907 - val_loss: 0.0120 - val_accuracy: 0.9316 - val_precision: 0.7716 - val_recall: 0.8689 - lr: 3.5000e-05\n",
      "Epoch 14/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 0.9323 - precision: 0.8280 - recall: 0.7782  \n",
      "Epoch 14: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 64s 374ms/step - loss: 0.0128 - accuracy: 0.9323 - precision: 0.8280 - recall: 0.7782 - val_loss: 0.0111 - val_accuracy: 0.9368 - val_precision: 0.8173 - val_recall: 0.8252 - lr: 3.5000e-05\n",
      "Epoch 15/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9323 - precision: 0.8195 - recall: 0.7907  \n",
      "Epoch 15: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 67s 391ms/step - loss: 0.0122 - accuracy: 0.9323 - precision: 0.8195 - recall: 0.7907 - val_loss: 0.0163 - val_accuracy: 0.9120 - val_precision: 0.6914 - val_recall: 0.9029 - lr: 3.5000e-05\n",
      "Epoch 16/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0119 - accuracy: 0.9341 - precision: 0.8263 - recall: 0.7938  \n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 1.2249999053892678e-05.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 62s 365ms/step - loss: 0.0119 - accuracy: 0.9341 - precision: 0.8263 - recall: 0.7938 - val_loss: 0.0128 - val_accuracy: 0.9316 - val_precision: 0.9315 - val_recall: 0.6602 - lr: 3.5000e-05\n",
      "Epoch 17/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9367 - precision: 0.8390 - recall: 0.7938  \n",
      "Epoch 17: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 63s 371ms/step - loss: 0.0122 - accuracy: 0.9367 - precision: 0.8390 - recall: 0.7938 - val_loss: 0.0123 - val_accuracy: 0.9376 - val_precision: 0.9290 - val_recall: 0.6990 - lr: 1.2250e-05\n",
      "Epoch 18/50\n",
      "Restoring model weights from the end of the best epoch: 11.0.0122 - accuracy: 0.9356 - precision: 0.8357 - recall: 0.7907  \n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 61s 356ms/step - loss: 0.0122 - accuracy: 0.9356 - precision: 0.8357 - recall: 0.7907 - val_loss: 0.0112 - val_accuracy: 0.9427 - val_precision: 0.8757 - val_recall: 0.7864 - lr: 1.2250e-05\n",
      "Epoch 18: early stopping\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(val_generator),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint_callback, nan_terminator]\n",
    ")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unfreeze some layers in the base model for fine-tuning\n",
    "base_model.trainable = True\n",
    "\n",
    "# Optionally, freeze the earlier layers to avoid overfitting\n",
    "fine_tune_at = 14  # Freeze layers from 0 to 14 and fine-tune layers 15 onward\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss=focal_loss(alpha=0.5, gamma=3.0),\n",
    "              metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9200 - precision_1: 0.7833 - recall_1: 0.7565   \n",
      "Epoch 1: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 66s 366ms/step - loss: 0.0167 - accuracy: 0.9200 - precision_1: 0.7833 - recall_1: 0.7565 - val_loss: 0.0130 - val_accuracy: 0.9308 - val_precision_1: 0.9699 - val_recall_1: 0.6262 - lr: 1.0000e-05\n",
      "Epoch 2/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0134 - accuracy: 0.9394 - precision_1: 0.8484 - recall_1: 0.8000  \n",
      "Epoch 2: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 65s 381ms/step - loss: 0.0134 - accuracy: 0.9394 - precision_1: 0.8484 - recall_1: 0.8000 - val_loss: 0.2680 - val_accuracy: 0.3205 - val_precision_1: 0.2058 - val_recall_1: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 3/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0103 - accuracy: 0.9502 - precision_1: 0.8682 - recall_1: 0.8466  \n",
      "Epoch 3: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 64s 371ms/step - loss: 0.0103 - accuracy: 0.9502 - precision_1: 0.8682 - recall_1: 0.8466 - val_loss: 0.0123 - val_accuracy: 0.9325 - val_precision_1: 0.9704 - val_recall_1: 0.6359 - lr: 1.0000e-05\n",
      "Epoch 4/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0087 - accuracy: 0.9564 - precision_1: 0.8783 - recall_1: 0.8746  \n",
      "Epoch 4: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 62s 360ms/step - loss: 0.0087 - accuracy: 0.9564 - precision_1: 0.8783 - recall_1: 0.8746 - val_loss: 0.0124 - val_accuracy: 0.9342 - val_precision_1: 0.7722 - val_recall_1: 0.8883 - lr: 1.0000e-05\n",
      "Epoch 5/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9577 - precision_1: 0.8879 - recall_1: 0.8705  \n",
      "Epoch 5: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 65s 377ms/step - loss: 0.0081 - accuracy: 0.9577 - precision_1: 0.8879 - recall_1: 0.8705 - val_loss: 0.0339 - val_accuracy: 0.8333 - val_precision_1: 0.5138 - val_recall_1: 0.9951 - lr: 1.0000e-05\n",
      "Epoch 6/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0076 - accuracy: 0.9619 - precision_1: 0.8955 - recall_1: 0.8881  \n",
      "Epoch 6: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 62s 362ms/step - loss: 0.0076 - accuracy: 0.9619 - precision_1: 0.8955 - recall_1: 0.8881 - val_loss: 0.0248 - val_accuracy: 0.9060 - val_precision_1: 0.9800 - val_recall_1: 0.4757 - lr: 1.0000e-05\n",
      "Epoch 7/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0063 - accuracy: 0.9649 - precision_1: 0.9039 - recall_1: 0.8964  \n",
      "Epoch 7: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 63s 366ms/step - loss: 0.0063 - accuracy: 0.9649 - precision_1: 0.9039 - recall_1: 0.8964 - val_loss: 0.0316 - val_accuracy: 0.8564 - val_precision_1: 0.5514 - val_recall_1: 0.9903 - lr: 1.0000e-05\n",
      "Epoch 8/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9729 - precision_1: 0.9190 - recall_1: 0.9285      \n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 3.4999999115825627e-06.\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.01100\n",
      "171/171 [==============================] - 59s 343ms/step - loss: 0.0057 - accuracy: 0.9729 - precision_1: 0.9190 - recall_1: 0.9285 - val_loss: 0.0134 - val_accuracy: 0.9564 - val_precision_1: 0.8507 - val_recall_1: 0.9126 - lr: 1.0000e-05\n",
      "Epoch 9/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0050 - accuracy: 0.9742 - precision_1: 0.9265 - recall_1: 0.9275  \n",
      "Epoch 9: val_loss improved from 0.01100 to 0.00928, saving model to checkpoints\\model_09.h5\n",
      "171/171 [==============================] - 61s 355ms/step - loss: 0.0050 - accuracy: 0.9742 - precision_1: 0.9265 - recall_1: 0.9275 - val_loss: 0.0093 - val_accuracy: 0.9530 - val_precision_1: 0.9521 - val_recall_1: 0.7718 - lr: 3.5000e-06\n",
      "Epoch 10/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9789 - precision_1: 0.9427 - recall_1: 0.9378  \n",
      "Epoch 10: val_loss improved from 0.00928 to 0.00798, saving model to checkpoints\\model_10.h5\n",
      "171/171 [==============================] - 60s 350ms/step - loss: 0.0042 - accuracy: 0.9789 - precision_1: 0.9427 - recall_1: 0.9378 - val_loss: 0.0080 - val_accuracy: 0.9650 - val_precision_1: 0.8986 - val_recall_1: 0.9029 - lr: 3.5000e-06\n",
      "Epoch 11/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0038 - accuracy: 0.9817 - precision_1: 0.9482 - recall_1: 0.9482      \n",
      "Epoch 11: val_loss did not improve from 0.00798\n",
      "171/171 [==============================] - 60s 348ms/step - loss: 0.0038 - accuracy: 0.9817 - precision_1: 0.9482 - recall_1: 0.9482 - val_loss: 0.0087 - val_accuracy: 0.9650 - val_precision_1: 0.9412 - val_recall_1: 0.8544 - lr: 3.5000e-06\n",
      "Epoch 12/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 0.9811 - precision_1: 0.9452 - recall_1: 0.9482  \n",
      "Epoch 12: val_loss improved from 0.00798 to 0.00772, saving model to checkpoints\\model_12.h5\n",
      "171/171 [==============================] - 60s 349ms/step - loss: 0.0036 - accuracy: 0.9811 - precision_1: 0.9452 - recall_1: 0.9482 - val_loss: 0.0077 - val_accuracy: 0.9675 - val_precision_1: 0.9242 - val_recall_1: 0.8883 - lr: 3.5000e-06\n",
      "Epoch 13/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.9863 - precision_1: 0.9607 - recall_1: 0.9617  \n",
      "Epoch 13: val_loss did not improve from 0.00772\n",
      "171/171 [==============================] - 59s 345ms/step - loss: 0.0033 - accuracy: 0.9863 - precision_1: 0.9607 - recall_1: 0.9617 - val_loss: 0.0092 - val_accuracy: 0.9658 - val_precision_1: 0.8990 - val_recall_1: 0.9078 - lr: 3.5000e-06\n",
      "Epoch 14/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9828 - precision_1: 0.9532 - recall_1: 0.9492      \n",
      "Epoch 14: val_loss did not improve from 0.00772\n",
      "171/171 [==============================] - 59s 344ms/step - loss: 0.0032 - accuracy: 0.9828 - precision_1: 0.9532 - recall_1: 0.9492 - val_loss: 0.0103 - val_accuracy: 0.9641 - val_precision_1: 0.8761 - val_recall_1: 0.9272 - lr: 3.5000e-06\n",
      "Epoch 15/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.9819 - precision_1: 0.9436 - recall_1: 0.9544  \n",
      "Epoch 15: val_loss did not improve from 0.00772\n",
      "171/171 [==============================] - 59s 344ms/step - loss: 0.0034 - accuracy: 0.9819 - precision_1: 0.9436 - recall_1: 0.9544 - val_loss: 0.0161 - val_accuracy: 0.9427 - val_precision_1: 0.7747 - val_recall_1: 0.9515 - lr: 3.5000e-06\n",
      "Epoch 16/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9846 - precision_1: 0.9490 - recall_1: 0.9648  \n",
      "Epoch 16: val_loss did not improve from 0.00772\n",
      "171/171 [==============================] - 59s 343ms/step - loss: 0.0032 - accuracy: 0.9846 - precision_1: 0.9490 - recall_1: 0.9648 - val_loss: 0.0118 - val_accuracy: 0.9504 - val_precision_1: 0.9625 - val_recall_1: 0.7476 - lr: 3.5000e-06\n",
      "Epoch 17/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0032 - accuracy: 0.9824 - precision_1: 0.9402 - recall_1: 0.9617      \n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 1.2249999372215823e-06.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.00772\n",
      "171/171 [==============================] - 59s 345ms/step - loss: 0.0032 - accuracy: 0.9824 - precision_1: 0.9402 - recall_1: 0.9617 - val_loss: 0.0117 - val_accuracy: 0.9607 - val_precision_1: 0.8478 - val_recall_1: 0.9466 - lr: 3.5000e-06\n",
      "Epoch 18/50\n",
      "171/171 [==============================] - ETA: 0s - loss: 0.0024 - accuracy: 0.9888 - precision_1: 0.9660 - recall_1: 0.9710     \n",
      "Epoch 18: val_loss did not improve from 0.00772\n",
      "171/171 [==============================] - 60s 353ms/step - loss: 0.0024 - accuracy: 0.9888 - precision_1: 0.9660 - recall_1: 0.9710 - val_loss: 0.0096 - val_accuracy: 0.9684 - val_precision_1: 0.9204 - val_recall_1: 0.8981 - lr: 1.2250e-06\n",
      "Epoch 19/50\n",
      "Restoring model weights from the end of the best epoch: 12.0.0021 - accuracy: 0.9910 - precision_1: 0.9664 - recall_1: 0.9834  \n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.00772\n",
      "171/171 [==============================] - 60s 352ms/step - loss: 0.0021 - accuracy: 0.9910 - precision_1: 0.9664 - recall_1: 0.9834 - val_loss: 0.0110 - val_accuracy: 0.9658 - val_precision_1: 0.8739 - val_recall_1: 0.9417 - lr: 1.2250e-06\n",
      "Epoch 19: early stopping\n"
     ]
    }
   ],
   "source": [
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),  # Use predefined dataset size\n",
    "    epochs=50,  \n",
    "    validation_data=val_generator,  # Use val_generator instead\n",
    "    validation_steps=len(val_generator),\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint_callback, nan_terminator]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Test Data...\n",
      "37/37 [==============================] - 11s 289ms/step - loss: 0.0102 - accuracy: 0.9620 - precision_1: 0.8900 - recall_1: 0.8942  \n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating on Test Data...\")\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(\n",
    "    test_generator,\n",
    "    steps=len(test_generator)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0102\n",
      "Test Accuracy: 96.20%\n",
      "Test Precision: 89.00%\n",
      "Test Recall: 89.42%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Test Precision: {test_precision * 100:.2f}%\")\n",
    "print(f\"Test Recall: {test_recall * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score: 89.21%\n"
     ]
    }
   ],
   "source": [
    "# Compute F1 Score\n",
    "if (test_precision + test_recall) > 0:\n",
    "    test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "else:\n",
    "    test_f1 = 0\n",
    "print(f\"Test F1 Score: {test_f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 11s 283ms/step\n"
     ]
    }
   ],
   "source": [
    "true_labels = []\n",
    "for i in range(len(test_generator)):\n",
    "    _, labels = test_generator[i]  # Extract batch labels\n",
    "    true_labels.extend(labels)\n",
    "\n",
    "true_labels = np.array(true_labels)  # Convert list to NumPy array\n",
    "\n",
    "test_predictions = model.predict(test_generator)\n",
    "test_predictions = test_predictions.flatten()  # Ensure correct shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.4648\n"
     ]
    }
   ],
   "source": [
    "precisions, recalls, thresholds = precision_recall_curve(true_labels, test_predictions)\n",
    "\n",
    "f1_scores = (2 * precisions * recalls) / (precisions + recalls + 1e-8)  # Avoid division by zero\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "print(f\"Optimal threshold: {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 10s 277ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "steps = len(test_generator)  # Ensure consistency\n",
    "test_predictions = model.predict(test_generator, steps=steps)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "threshold = best_threshold\n",
    "predicted_classes = (test_predictions > threshold).astype(int)\n",
    "\n",
    "# Trim true labels to match predictions\n",
    "true_labels = np.array(test_generator.labels)\n",
    "\n",
    "# Ensure no mismatches\n",
    "assert len(true_labels) == len(predicted_classes), \"Mismatch in true labels and predictions!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#for debugginfg purpose only\n",
    "# true_labels = test_generator.labels  # Check if this exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#for debugginfg purpose only\n",
    "# print(len(true_labels))  \n",
    "# print(len(predicted_classes))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of true_labels: (1184,)\n",
      "Shape of predicted_classes: (1184, 1)\n"
     ]
    }
   ],
   "source": [
    "true_labels = np.array(true_labels)  # Convert list to NumPy array\n",
    "predicted_classes = np.array(predicted_classes)  # Ensure predictions are also an array\n",
    "\n",
    "print(f\"Shape of true_labels: {true_labels.shape}\")  \n",
    "print(f\"Shape of predicted_classes: {predicted_classes.shape}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of true_labels: (1184,)\n",
      "Shape of predicted_classes: (1184, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of true_labels: {true_labels.shape}\")  \n",
    "print(f\"Shape of predicted_classes: {predicted_classes.shape}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#for debugging purposes \n",
    "# print(type(true_labels))  # Check the type of true_labels\n",
    "# print(true_labels)  # Print its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(true_labels, predicted_classes)\n",
    "class_labels = [\"au\", \"tp\"]  # Adjust according to your dataset\\\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          au       0.98      0.98      0.98       976\n",
      "          tp       0.89      0.92      0.91       208\n",
      "\n",
      "    accuracy                           0.97      1184\n",
      "   macro avg       0.94      0.95      0.94      1184\n",
      "weighted avg       0.97      0.97      0.97      1184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report:\")\n",
    "class_labels = [\"au\", \"tp\"]  # Adjust according to your dataset\n",
    "print(classification_report(true_labels, predicted_classes, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(true_labels, predicted_classes, zero_division=1)\n",
    "recall = recall_score(true_labels, predicted_classes, zero_division=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8889\n",
      "Recall: 0.9231\n"
     ]
    }
   ],
   "source": [
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in predictions: (array([0, 1]), array([968, 216], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution in predictions:\", np.unique(predicted_classes, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"models/v4.h5\")\n",
    "model.save(\"models/v4.keras\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
